{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q learning",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanyaanand/deep-learning/blob/master/q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDLrVgxAPSnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04F_xwJeOloq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "e44b816e-e55b-49b1-ca82-63a4e25f16be"
      },
      "source": [
        "!pip install tensorflow==1.13.2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/d3/651f95288a6cd9094f7411cdd90ef12a3d01a268009e0e3cd66b5c8d65bd/tensorflow-1.13.2-cp36-cp36m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 43kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 38.8MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.31.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.1.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.13.2) (1.18.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.2) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.0.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.2) (49.2.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.2) (3.1.0)\n",
            "Installing collected packages: keras-applications, tensorboard, mock, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.2 tensorboard-1.13.1 tensorflow-1.13.2 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z2RSzkhPaZq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "ec6d8cf5-3dd7-4e61-b823-30432dff7e8d"
      },
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.autograd import Variable\n",
        "import calendar \n",
        "!pip install gym\n",
        "!pip install stable-baselines"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.2)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Collecting stable-baselines\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/48/d428b79bd4360727925f9fe34afeea7a9da381da3dc8748df834a349ad1d/stable_baselines-2.10.1-py3-none-any.whl (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (3.2.2)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.3.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.4.1)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (0.17.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines) (1.0.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines) (2.4.7)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (0.2.6)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines) (7.0.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines) (2018.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->stable-baselines) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines) (0.16.0)\n",
            "Installing collected packages: stable-baselines\n",
            "Successfully installed stable-baselines-2.10.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAXNyMBdPcjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.memory = []\n",
        "        self.position = 0\n",
        "\n",
        "    def push(self, *args):\n",
        "        \"\"\"Saves a transition.\"\"\"\n",
        "        if len(self.memory) < self.capacity:\n",
        "            self.memory.append(None)\n",
        "        self.memory[self.position] = Transition(*args)\n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr6oNRrtSwqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_dims, output_dims):\n",
        "        \"\"\"Initialize a decoder in DA_RNN.\"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.input_size = input_size\n",
        "        self.output_dims = output_dims\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.input_size, self.hidden_dims),\n",
        "            nn.BatchNorm1d(self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.hidden_dims, 2*self.hidden_dims),\n",
        "            nn.BatchNorm1d(2*self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*self.hidden_dims, 2*self.hidden_dims),\n",
        "            nn.BatchNorm1d(2*self.hidden_dims),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2*self.hidden_dims, self.hidden_dims),\n",
        "            nn.BatchNorm1d(self.hidden_dims),\n",
        "            nn.ReLU(),                        \n",
        "        )\n",
        "        print(self.fc)\n",
        "        self.q_value = nn.Sequential(\n",
        "            nn.Linear(self.hidden_dims, self.output_dims),\n",
        "            nn.BatchNorm1d(self.output_dims),\n",
        "            nn.Softmax(-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"forward.\"\"\"      \n",
        "\n",
        "        x = self.fc(x)\n",
        "        q_Action = self.q_value(x)\n",
        "        return q_Action\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EjthxtopB1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from gym.utils import seeding\n",
        "import gym\n",
        "import csv\n",
        "from gym import spaces\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from calendar import isleap\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
        "# player mode - slightly mor advance with more functinality\n",
        "\n",
        "\n",
        "class StockEnvPlayer(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}\n",
        "\n",
        "    def __init__(self, df, interval, mode = 'Train', logfile = './Logger.txt', modelName='q-learning', ntimestep = 20, initial_investment=100000, seed=7, commission=1.0, addTA=False):\n",
        "        super(StockEnvPlayer, self).__init__()\n",
        "        self.addTA = addTA\n",
        "        self.interval = interval\n",
        "        self.X = np.zeros([ntimestep, len(interval)])\n",
        "        self.mode = mode\n",
        "        self.done = False\n",
        "        self.closing_price = []\n",
        "        self.bought = []\n",
        "        self.sold = []\n",
        "        self.df = df\n",
        "        self.leg = ntimestep\n",
        "        self.episode_reward = 0\n",
        "        self.G = []\n",
        "        self.lastsell = 0\n",
        "        self.possell = 0\n",
        "        self.negsell = 0\n",
        "        self.lastbuy = 0 \n",
        "        self.action_validation = np.zeros(len(interval) + 1) # add 1 for p_t-1\n",
        "        self.avg_closing = np.zeros(len(interval))\n",
        "        self.ntimestep = ntimestep\n",
        "        self.dates = df.date# number of samples \n",
        "        self.numSecurity = 1 # how many stock are there in a portfolio in our case two\n",
        "        self.numTrainDay = len(self.df)\n",
        "        self.terminal = False\n",
        "        self.commission = float(commission)*-1\n",
        "        self.inventory = []\n",
        "        self.initial_investment = initial_investment\n",
        "        self.logfile = logfile\n",
        "        self.i = 0\n",
        "        self.modelName = modelName\n",
        "        self._seed(seed)\n",
        "        date = list(self.df.date)\n",
        "        year = []\n",
        "        month = []\n",
        "        day = []\n",
        "        weekday = []\n",
        "        for i in date:\n",
        "          t = i.split(\"/\")\n",
        "          year.append(float(t[0]))\n",
        "          month.append(float(t[1]))\n",
        "          day.append(float(t[2]))\n",
        "          weekday.append(self.findDay(i))\n",
        "        self.df.loc[:, 'year'] = np.array(year)\n",
        "        self.df.loc[:, 'month'] = np.array(month)\n",
        "        self.df.loc[:, 'date'] = np.array(day)\n",
        "        self.df.loc[:, 'day'] = np.array(weekday)\n",
        "        self.TA_columns = ['Value(Lk)', 'Avg3(E,25,E,13,E,8)-Avg1',\n",
        "       'Avg3(E,25,E,13,E,8)-Avg2', 'Avg3(E,25,E,13,E,8)-Avg3',\n",
        "       'year', 'month', 'date', 'day', 'hour', 'minute', 'second']\n",
        "        self.action_space = spaces.Box(\n",
        "            low=0, high=2, shape=(1, ), dtype=np.float16)# change\n",
        "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(ntimestep*len(self.TA_columns)+(len(self.interval)+3)*self.ntimestep, ))\n",
        "        print('observation_space :\\t', self.observation_space)\n",
        "        print('action_space :\\t', self.action_space)\n",
        "        self.reset()\n",
        "\n",
        "        # write column header for the first time\n",
        "        with open(logfile, 'a+') as f:\n",
        "            ap = ['asset' + str(i) + '_price' for i in range(self.numSecurity)]\n",
        "            aq = ['asset' + str(i) + '_qty' for i in range(self.numSecurity)]\n",
        "            others = ['asset' + str(i) + '_' + col for i in range(self.numSecurity)\n",
        "                      for col in self.TA_columns]\n",
        "            column = 'model, incTA?, step, date, cash, portfolio, reward, total_commission, buy_amt, buy_commission, sell_amt, sell_commission,' + \\\n",
        "                ','.join(ap) + ',' + ','.join(aq) + ',' + '\\n'\n",
        "            f.write(column)\n",
        "\n",
        "    def findDay(self, t): \n",
        "        year, month, day = t.split('/')     \n",
        "        dayNumber = calendar.weekday(int(year), int(month), int(day)) \n",
        "        return float(dayNumber)\n",
        "\n",
        "    def pastdata(self):\n",
        "      self.X = np.zeros([self.ntimestep, len(interval)])\n",
        "      for t in range(len(self.interval)):\n",
        "        time = self.interval[t]\n",
        "        index = []\n",
        "        i = self.leg - time - 1\n",
        "        l = 0\n",
        "        while True:\n",
        "          if i < 0 or l == self.ntimestep:\n",
        "            break\n",
        "          else:\n",
        "            l +=1\n",
        "            index.append(self.df.loc[i, 'Avg3(E,25,E,13,E,8)-Avg1'])\n",
        "          i-=time\n",
        "        if len(index) != 0: \n",
        "          self.X[:len(index), t] = np.array(index)[::-1]\n",
        "    \n",
        "\n",
        "    def reset(self):\n",
        "\n",
        "        self.portfolio_value = [self.initial_investment] # check point \n",
        "        self.net_cash = [self.initial_investment]\n",
        "        self.leg = self.ntimestep\n",
        "        self.X = np.zeros([self.ntimestep, len(interval)])\n",
        "        self.ledger = []\n",
        "        self.lastsell = 0\n",
        "        self.possell = 0\n",
        "        self.negsell = 0\n",
        "        self.lastbuy = 0 \n",
        "        self.action_validation = np.zeros(len(interval) + 1) # add 1 for p_t-1\n",
        "        self.avg_closing = np.zeros(len(interval))        \n",
        "        self.i = 0\n",
        "        self.terminal = False\n",
        "        self.done = False\n",
        "        self.reward = 0\n",
        "        self.df.loc[:, 'qty'] = 0.0\n",
        "        self.df.loc[:self.ntimestep, 'initial'] = self.initial_investment ###check\n",
        "        self.commission_paid = [0] * len(self.dates)\n",
        "        self.transaction = {\"buy_amt\": 0, \"buy_commission\": 0,\n",
        "                            \"sell_amt\": 0, \"sell_commission\": 0, }\n",
        "\n",
        "        # return as range of one day else it becomes a panda series\n",
        "        self.data = self.df[self.leg-self.ntimestep:self.leg]\n",
        "        self.value = self.data.loc[:, ['initial']].values# initial investment\n",
        "        self.price = self.data.loc[:, ['Close']].values\n",
        "        self.qty = self.data.loc[:, ['qty']].values\n",
        "        self.ta = self.data.loc[:, self.TA_columns].values.reshape(-1, 1)\n",
        "        self.state = np.concatenate((self.value, self.price, self.qty, self.ta, self.X.reshape(-1,1)))\n",
        "\n",
        "        return np.squeeze(self.state)\n",
        "\n",
        "    def _sell_stock(self):\n",
        "        if self.qty[-1] > 0:\n",
        "            self.lastsell = 0\n",
        "            self.sold.append(self.i)\n",
        "            # quantity = min(abs(action), self.qty[index])\n",
        "            quantity = 1.0\n",
        "            sell_amt = self.price[-1] * quantity\n",
        "            self.transaction[\"sell_amt\"] += sell_amt\n",
        "            self.transaction[\"sell_commission\"] += sell_amt * self.commission\n",
        "            self.commission_paid[self.leg] += sell_amt * self.commission\n",
        "            self.value[:-1] = self.value[1:]\n",
        "            self.value[-1]+= sell_amt + (self.commission)\n",
        "            self.qty[:-1] = self.qty[1:]\n",
        "            self.qty[-1] -= quantity\n",
        "            if self.qty[-1] == 0.0:\n",
        "              self.done = True\n",
        "              self.G.append(self.episode_reward)\n",
        "              self.episode_reward = 0\n",
        "            # update investment and qty\n",
        "            self.state = np.concatenate((self.value, self.price, self.qty, self.ta, self.X.reshape(-1,1)))\n",
        "\n",
        "        else:\n",
        "            self.dont = True\n",
        "            self.episode_reward -= 10\n",
        "            self.reward = -10\n",
        "            # print(\"No asset to sell\")\n",
        "            pass\n",
        "\n",
        "    def _buy_stock(self):\n",
        "        min_quantity = self.value[-1] // self.price[-1] # max qauntity which can be purchased\n",
        "        quantity = min(min_quantity, 1.0)\n",
        "        if quantity > 0:\n",
        "          # print(\"Bought at price {}\".format(self.price[-1]))\n",
        "          self.bought.append(self.i)\n",
        "          buy_amt = self.price[-1] * quantity\n",
        "          # print(\"buy\", self.dates[self.day], index, action, quantity)\n",
        "          self.inventory.append(buy_amt)\n",
        "          self.transaction[\"buy_amt\"] += buy_amt\n",
        "          self.transaction[\"buy_commission\"] += buy_amt * self.commission\n",
        "\n",
        "          self.commission_paid[self.leg] += buy_amt * self.commission\n",
        "          self.value[:-1] = self.value[1:]\n",
        "          self.value[-1] -= (buy_amt - (self.commission))# check point; check for proper buying commission\n",
        "\n",
        "          self.qty[:-1] = self.qty[1:]\n",
        "          self.qty[-1] += quantity\n",
        "          # update investment and qty\n",
        "          self.state = np.concatenate((self.value, self.price, self.qty, self.ta, self.X.reshape(-1,1)))\n",
        "\n",
        "\n",
        "        else:\n",
        "          self.dont = True\n",
        "          self.episode_reward -= 10\n",
        "          # print(\"No cash to buy\")\n",
        "          self.reward = -10\n",
        "          pass\n",
        "\n",
        "    def step(self, actions):\n",
        "        self.dont = False\n",
        "        self.done = False\n",
        "        self.terminal = self.leg >= (self.numTrainDay-1)\n",
        "        self.transaction = {\"buy_amt\": 0, \"buy_commission\": 0,\n",
        "                            \"sell_amt\": 0, \"sell_commission\": 0, }# check point\n",
        "\n",
        "        if self.terminal:\n",
        "            print(\"**** Summary*****\")\n",
        "            print(\"Model:\\t\\t\\t\", self.modelName.upper())\n",
        "            print(\"Number of Assets:\\t{:8.0f}\".format(self.numSecurity))\n",
        "            print(\"Initial Investment :\\t{:8.2f}\".format(self.initial_investment))\n",
        "            portfolio_value = self.value[-1] + (self.price[-1] * self.qty[-1])\n",
        "            rtns_dollar = np.round(portfolio_value[0] - self.initial_investment, 2)\n",
        "            rtns_pct = np.round((portfolio_value[0]/self.initial_investment-1)*100, 2)\n",
        "            # rtns_annualised = (1+rtns_pct) ** (1/self.years)-1\n",
        "            print(\"Portfolio Value:\\t{:8.2f}\".format(portfolio_value[0]))\n",
        "            print(\"% Returns:\\t\\t{:8.2f}%\".format(rtns_pct))\n",
        "            print(\"***************\")\n",
        "            # fig, ax = plt.subplots()\n",
        "            # ax.set_title(self.modelName)\n",
        "            # ax.set_ylabel('Total Asset $')\n",
        "            # ax.set_xlabel('Episode')\n",
        "            # ax.plot(self.portfolio_value, color='tomato')\n",
        "            # plt.savefig('image/{}.png'.format(self.modelName))\n",
        "            # plt.close()\n",
        "            with open(self.logfile, 'a+') as myfile:\n",
        "                wr = csv.writer(myfile)\n",
        "                wr.writerows(self.ledger)\n",
        "            print(\"Terminal state reseting the environment\")\n",
        "            self.render()\n",
        "        else:\n",
        "            begin_total_asset = self.value[-1] + (self.price[-1] * self.qty[-1])\n",
        "            begin_cash = self.value[-1]\n",
        "            self.closing_price.append(self.price[-1])\n",
        "            \"\"\"\n",
        "              Action 0 : buy\n",
        "              Action 1 : hold\n",
        "              Action 2 : sell\n",
        "            \"\"\"\n",
        "            if actions == 2:\n",
        "              self.lastbuy+=1\n",
        "              self._sell_stock()\n",
        "            elif actions == 1:\n",
        "              self.lastsell += 1\n",
        "              self.possell += 1\n",
        "              self.negsell += 1\n",
        "              self._buy_stock()       \n",
        "            else:\n",
        "              self.lastsell += 1\n",
        "              self.possell += 1\n",
        "              self.negsell += 1\n",
        "              self.lastbuy += 1\n",
        "              self.value[:-1] = self.value[1:]\n",
        "              self.value[-1] = self.value[-1]\n",
        "              self.qty[:-1] = self.qty[1:]\n",
        "              self.qty[-1] = self.qty[-1]\n",
        "              \n",
        "            # get next day's price & ta\n",
        "            self.leg += 1\n",
        "            self.i+=1\n",
        "            self.data = self.df[self.leg - self.ntimestep:self.leg]\n",
        "            self.price = self.data.loc[:, ['Close']].values\n",
        "            self.ta = self.data.loc[:, self.TA_columns].values.reshape(-1, 1)           \n",
        "            self.pastdata()\n",
        "            self.state = np.concatenate((self.value, self.price, self.qty, self.ta, self.X.reshape(-1,1)), axis = 0)\n",
        "            end_total_asset = self.value[-1] + (self.price[-1] * self.qty[-1])\n",
        "            self.episode_reward +=(end_total_asset - begin_total_asset)[0]\n",
        "            if not self.dont:            \n",
        "              self.reward = (end_total_asset - begin_total_asset)[0]         \n",
        "              self.\n",
        "            else:\n",
        "              self.value[:-1] = self.value[1:]\n",
        "              self.value[-1] = self.value[-1]\n",
        "              self.qty[:-1] = self.qty[1:]\n",
        "              self.qty[-1] = self.qty[-1]                \n",
        "            self.portfolio_value.append((end_total_asset))\n",
        "            self.G.append((end_total_asset - begin_total_asset)[0])\n",
        "        return np.squeeze(self.state).copy(), self.reward, self.terminal, {}\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        line = np.array([self.modelName, self.addTA, self.leg, str(self.dates[self.leg]), str(\n",
        "            self.value[-1]), str(self.portfolio_value[-1]), self.reward,\n",
        "            self.commission_paid[self.leg-1],\n",
        "            self.transaction[\"buy_amt\"], self.transaction[\"buy_commission\"],\n",
        "            self.transaction[\"sell_amt\"], self.transaction[\"sell_commission\"]\n",
        "        ])\n",
        "        display = np.concatenate((line, self.price[-1], self.qty[-1]))\n",
        "        self.ledger.append(display)\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.set_title(self.modelName)\n",
        "        ax.set_ylabel('Total Asset $')\n",
        "        ax.set_xlabel('Episode')\n",
        "        ax.plot(self.portfolio_value, color='tomato')\n",
        "        fig = plt.figure(figsize = (15,5))\n",
        "        plt.plot(self.closing_price, color='gold', lw=2.)\n",
        "        plt.plot(self.closing_price, '*', markersize=6, color='r', label = 'buying signal', markevery = self.bought)\n",
        "        plt.plot(self.closing_price, 'o', markersize=6, color='g', label = 'selling signal', markevery = self.sold)\n",
        "        plt.title('{} reward {} total capital {} # buying {} # selling {}'.format(self.mode, self.episode_reward, self.value[-1], len(self.bought), len(self.sold)))\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        self.i = 0\n",
        "        self.closing_price = []\n",
        "        self.bought = []\n",
        "        self.sold = []\n",
        "        self.episode_reward = 0\n",
        "    def _seed(self, seed=None):\n",
        "        self.np_random, seed = seeding.np_random(seed)\n",
        "        return [seed]"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QILKWMbQoQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def optimize_model():\n",
        "    if len(memory) < BATCH_SIZE:\n",
        "        return\n",
        "    transitions = memory.sample(BATCH_SIZE)\n",
        "    policy_net.train()\n",
        "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
        "    # detailed explanation). This converts batch-array of Transitions\n",
        "    # to Transition of batch-arrays.\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Compute a mask of non-final states and concatenate the batch elements\n",
        "    # (a final state would've been the one after which simulation ended)\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
        "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
        "                                                if s is not None])\n",
        "\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
        "    # columns of actions taken. These are the actions which would've been taken\n",
        "    # for each batch state according to policy_net\n",
        "    state_action_values = policy_net(state_batch.float()).gather(1, action_batch)\n",
        "\n",
        "    # Compute V(s_{t+1}) for all next states.\n",
        "    # Expected values of actions for non_final_next_states are computed based\n",
        "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
        "    # This is merged based on the mask, such that we'll have either the expected\n",
        "    # state value or 0 in case the state was final.\n",
        "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "    next_state_values[non_final_mask] = target_net(non_final_next_states.float()).max(1)[0].detach()\n",
        "    # Compute the expected Q values\n",
        "    expected_state_action_values = (next_state_values.unsqueeze(1) * GAMMA) + reward_batch\n",
        "    # Compute Huber loss\n",
        "    loss = F.smooth_l1_loss(state_action_values.float(), expected_state_action_values.float())\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "\n",
        "    # for param in policy_net.parameters():\n",
        "    #     print(param.grad.size())\n",
        "    #     print(param.grad)\n",
        "    #     # param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "    optimizer.step()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYiWA59lQrJ4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "c64c4552-63ca-45bd-9c4f-1c3892d9d950"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "GAMMA = 0.999\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.1\n",
        "EPS_DECAY = 400\n",
        "TARGET_UPDATE = 50000\n",
        "n_actions = 3\n",
        "T = 20\n",
        "interval = [2, 3, 6, 12, 16, 20]\n",
        "memory = ReplayMemory(20000)\n",
        "# Get screen size so that we can initialize layers correctly based on shape\n",
        "# returned from AI gym. Typical dimensions at this point are close to 3x40x90\n",
        "# which is the result of a clamped and down-scaled render buffer in get_screen()\n",
        "train = pd.read_csv(\"./train10min.csv\")\n",
        "train_env = DummyVecEnv(\n",
        "    [lambda: StockEnvPlayer(train, interval)])\n",
        "\n",
        "# Automatically normalize the input features\n",
        "train_env = VecNormalize(train_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
        "test = pd.read_csv(\"./test10min.csv\")\n",
        "test_env = DummyVecEnv(\n",
        "    [lambda: StockEnvPlayer(test, interval, mode='Test')])\n",
        "\n",
        "# Automatically normalize the input features\n",
        "test_env = VecNormalize(test_env, norm_obs=True, norm_reward=False, clip_obs=10.)\n",
        "\n",
        "state = train_env.reset()\n",
        "input_size = (state).shape[-1]\n",
        "print(input_size)\n",
        "policy_net = DQN(input_size, 64, n_actions).to(device)\n",
        "target_net = DQN(input_size, 64, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "target_net.eval()\n",
        "decayRate = 0.5\n",
        "lr = 0.001\n",
        "# optimizer = optim.RMSprop(policy_net.parameters())\n",
        "optimizer = torch.optim.Adam(params=policy_net.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state, mode = 'train'):\n",
        "    global steps_done\n",
        "    policy_net.eval()\n",
        "    if mode == 'train':\n",
        "      sample = random.random()\n",
        "      eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "          math.exp(-1. * steps_done / EPS_DECAY)\n",
        "      steps_done += 1\n",
        "\n",
        "      if sample > eps_threshold:\n",
        "          with torch.no_grad():\n",
        "              # t.max(1) will return largest column value of each row.\n",
        "              # second column on max result is index of where max element was\n",
        "              # found, so we pick action with the larger expected reward.\n",
        "              return policy_net(state.float()).max(1)[1].view(1, 1)\n",
        "      else:\n",
        "          return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        return policy_net(state.float()).max(1)[1].view(1, 1)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "commission -1.0\n",
            "observation_space :\t Box(480,)\n",
            "action_space :\t Box(1,)\n",
            "commission -1.0\n",
            "observation_space :\t Box(480,)\n",
            "action_space :\t Box(1,)\n",
            "480\n",
            "Sequential(\n",
            "  (0): Linear(in_features=480, out_features=64, bias=True)\n",
            "  (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): ReLU()\n",
            "  (9): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): ReLU()\n",
            ")\n",
            "Sequential(\n",
            "  (0): Linear(in_features=480, out_features=64, bias=True)\n",
            "  (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): ReLU()\n",
            "  (3): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (4): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (7): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): ReLU()\n",
            "  (9): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (11): ReLU()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POsxithtRUmA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_run():\n",
        "\n",
        "  ok = True\n",
        "  j = 0\n",
        "  total_reward = 0\n",
        "  state = test_env.reset()\n",
        "  while ok:     \n",
        "    state = torch.from_numpy(state).to(device)\n",
        "    action = select_action(state, mode=\"test\")\n",
        "    next_state, reward, terminate, info = test_env.step(action)\n",
        "    state = next_state\n",
        "    total_reward+=reward \n",
        "    j+=1\n",
        "    if terminate:\n",
        "      ok = False\n",
        "      break            \n",
        "  print(\"test\", total_reward[0])\n"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwL2Db-zQtm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_episode = 50\n",
        "# Initialize the environment and state\n",
        "\n",
        "for n_episode in range(max_episode):\n",
        "  state = train_env.reset()  \n",
        "  state = torch.from_numpy(state).to(device)  \n",
        "  timestep = 0\n",
        "  while True:\n",
        "    if timestep % TARGET_UPDATE == 0:\n",
        "      target_net.load_state_dict(policy_net.state_dict())        \n",
        "    # Select and perform an action\n",
        "    timestep+=1\n",
        "    print(state)\n",
        "    action = select_action(state)\n",
        "    next_state, reward, done, info = train_env.step(action)\n",
        "    next_state = torch.from_numpy(next_state).to(device)  \n",
        "    reward = torch.tensor([reward], device=device)\n",
        "\n",
        "    if done:\n",
        "      temp = next_state\n",
        "      next_state = None\n",
        "    # Store the transition in memory\n",
        "    memory.push(state, action, next_state, reward)      \n",
        "    if done:\n",
        "      state = temp\n",
        "    else:\n",
        "      state = next_state\n",
        "\n",
        "    # Perform one step of the optimization (on the target network)\n",
        "    optimize_model()\n",
        "    if done:\n",
        "      break\n",
        "    if timestep%5000 == 0:\n",
        "\n",
        "      lr_scheduler.step()\n",
        "      train_env.render()\n",
        "      test_run()   \n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}